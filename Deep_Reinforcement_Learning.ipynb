{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Deep Reinforcement Learning*"
      ],
      "metadata": {
        "id": "cWQ_O-DtEZ3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Install Dependencies"
      ],
      "metadata": {
        "id": "a9TOetesEUyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "!pip install gym \n",
        "!pip install keras\n",
        "!pip install keras-rl2\n",
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-N_geznEnmf",
        "outputId": "1cc948ee-0628-47c1-a30e-5ecba83d56b9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Test Random Environment with OpenAl Gym"
      ],
      "metadata": {
        "id": "bmuU3ss0Fjwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gym\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'"
      ],
      "metadata": {
        "id": "Cml5nowRFD_6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "states = env.observation_space.shape[0]\n",
        "print(\"number os states:\", states)\n",
        "actions = env.action_space.n\n",
        "print(\"number os actions:\", actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o4pE_PlGfbM",
        "outputId": "5a28cec0-33dd-4b2d-e9a3-014d1e118ef7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number os states: 4\n",
            "number os actions: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 10\n",
        "for episode in range(1, episodes+10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = random.choice([0, 1])\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "    print(\"Episode:{} Score:{}\".format(episode, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF4jQ_s9G1JU",
        "outputId": "397e89ca-6ec9-42b7-95de-0b037cd5b82a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:50: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"You are calling render method, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:39.0\n",
            "Episode:2 Score:19.0\n",
            "Episode:3 Score:12.0\n",
            "Episode:4 Score:15.0\n",
            "Episode:5 Score:34.0\n",
            "Episode:6 Score:14.0\n",
            "Episode:7 Score:14.0\n",
            "Episode:8 Score:34.0\n",
            "Episode:9 Score:15.0\n",
            "Episode:10 Score:15.0\n",
            "Episode:11 Score:16.0\n",
            "Episode:12 Score:26.0\n",
            "Episode:13 Score:25.0\n",
            "Episode:14 Score:11.0\n",
            "Episode:15 Score:17.0\n",
            "Episode:16 Score:35.0\n",
            "Episode:17 Score:11.0\n",
            "Episode:18 Score:9.0\n",
            "Episode:19 Score:27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Create a Deep Learning Model with Keras"
      ],
      "metadata": {
        "id": "6J9R4-C9K3L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import neccessary modules\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "6suHm1GwJEoX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1, states)))\n",
        "    model.add(Dense(units=24, activation='relu' ))\n",
        "    model.add(Dense(units=24, activation='relu'))\n",
        "    model.add(Dense(units=actions, activation='linear'))\n",
        "    return model\n",
        "    "
      ],
      "metadata": {
        "id": "dCm35tDdLd2u"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0BpGAnBT9tI",
        "outputId": "17c1742e-cce3-4b2e-b108-0c5337c97bf2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Build Agent with Keras-RL"
      ],
      "metadata": {
        "id": "rzsaGAQtUrbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory "
      ],
      "metadata": {
        "id": "CO9nH2OeUYm_"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn\n"
      ],
      "metadata": {
        "id": "tMxKthk5UlrD"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqy1Z7hBXU1I",
        "outputId": "64d6bdc9-3b43-441f-b9d3-8dd4f4dc53af"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: 1.0000\n",
            "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 16.368 - mae: 51.454 - mean_q: 103.278\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 127s 13ms/step - reward: 1.0000\n",
            "67 episodes - episode_reward: 149.104 [18.000, 200.000] - loss: 10.868 - mae: 42.239 - mean_q: 84.913\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 125s 12ms/step - reward: 1.0000\n",
            "104 episodes - episode_reward: 96.048 [14.000, 186.000] - loss: 16.295 - mae: 47.639 - mean_q: 95.629\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: 1.0000\n",
            "55 episodes - episode_reward: 181.982 [48.000, 200.000] - loss: 8.630 - mae: 38.222 - mean_q: 76.881\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 1.0000\n",
            "done, took 582.936 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3f50438f50>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4HECOo3X02e",
        "outputId": "cd55a784-367f-4abf-9d72-fbb8bc0ffeef"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n",
            "Episode 21: reward: 200.000, steps: 200\n",
            "Episode 22: reward: 200.000, steps: 200\n",
            "Episode 23: reward: 200.000, steps: 200\n",
            "Episode 24: reward: 200.000, steps: 200\n",
            "Episode 25: reward: 200.000, steps: 200\n",
            "Episode 26: reward: 200.000, steps: 200\n",
            "Episode 27: reward: 200.000, steps: 200\n",
            "Episode 28: reward: 200.000, steps: 200\n",
            "Episode 29: reward: 200.000, steps: 200\n",
            "Episode 30: reward: 200.000, steps: 200\n",
            "Episode 31: reward: 200.000, steps: 200\n",
            "Episode 32: reward: 200.000, steps: 200\n",
            "Episode 33: reward: 200.000, steps: 200\n",
            "Episode 34: reward: 200.000, steps: 200\n",
            "Episode 35: reward: 200.000, steps: 200\n",
            "Episode 36: reward: 200.000, steps: 200\n",
            "Episode 37: reward: 200.000, steps: 200\n",
            "Episode 38: reward: 200.000, steps: 200\n",
            "Episode 39: reward: 200.000, steps: 200\n",
            "Episode 40: reward: 200.000, steps: 200\n",
            "Episode 41: reward: 200.000, steps: 200\n",
            "Episode 42: reward: 200.000, steps: 200\n",
            "Episode 43: reward: 200.000, steps: 200\n",
            "Episode 44: reward: 200.000, steps: 200\n",
            "Episode 45: reward: 200.000, steps: 200\n",
            "Episode 46: reward: 200.000, steps: 200\n",
            "Episode 47: reward: 200.000, steps: 200\n",
            "Episode 48: reward: 200.000, steps: 200\n",
            "Episode 49: reward: 200.000, steps: 200\n",
            "Episode 50: reward: 200.000, steps: 200\n",
            "Episode 51: reward: 200.000, steps: 200\n",
            "Episode 52: reward: 200.000, steps: 200\n",
            "Episode 53: reward: 200.000, steps: 200\n",
            "Episode 54: reward: 200.000, steps: 200\n",
            "Episode 55: reward: 200.000, steps: 200\n",
            "Episode 56: reward: 200.000, steps: 200\n",
            "Episode 57: reward: 200.000, steps: 200\n",
            "Episode 58: reward: 200.000, steps: 200\n",
            "Episode 59: reward: 200.000, steps: 200\n",
            "Episode 60: reward: 200.000, steps: 200\n",
            "Episode 61: reward: 200.000, steps: 200\n",
            "Episode 62: reward: 200.000, steps: 200\n",
            "Episode 63: reward: 200.000, steps: 200\n",
            "Episode 64: reward: 200.000, steps: 200\n",
            "Episode 65: reward: 200.000, steps: 200\n",
            "Episode 66: reward: 200.000, steps: 200\n",
            "Episode 67: reward: 200.000, steps: 200\n",
            "Episode 68: reward: 200.000, steps: 200\n",
            "Episode 69: reward: 200.000, steps: 200\n",
            "Episode 70: reward: 200.000, steps: 200\n",
            "Episode 71: reward: 200.000, steps: 200\n",
            "Episode 72: reward: 200.000, steps: 200\n",
            "Episode 73: reward: 200.000, steps: 200\n",
            "Episode 74: reward: 200.000, steps: 200\n",
            "Episode 75: reward: 200.000, steps: 200\n",
            "Episode 76: reward: 200.000, steps: 200\n",
            "Episode 77: reward: 200.000, steps: 200\n",
            "Episode 78: reward: 200.000, steps: 200\n",
            "Episode 79: reward: 200.000, steps: 200\n",
            "Episode 80: reward: 200.000, steps: 200\n",
            "Episode 81: reward: 200.000, steps: 200\n",
            "Episode 82: reward: 200.000, steps: 200\n",
            "Episode 83: reward: 200.000, steps: 200\n",
            "Episode 84: reward: 200.000, steps: 200\n",
            "Episode 85: reward: 200.000, steps: 200\n",
            "Episode 86: reward: 200.000, steps: 200\n",
            "Episode 87: reward: 200.000, steps: 200\n",
            "Episode 88: reward: 200.000, steps: 200\n",
            "Episode 89: reward: 200.000, steps: 200\n",
            "Episode 90: reward: 200.000, steps: 200\n",
            "Episode 91: reward: 200.000, steps: 200\n",
            "Episode 92: reward: 200.000, steps: 200\n",
            "Episode 93: reward: 200.000, steps: 200\n",
            "Episode 94: reward: 200.000, steps: 200\n",
            "Episode 95: reward: 200.000, steps: 200\n",
            "Episode 96: reward: 200.000, steps: 200\n",
            "Episode 97: reward: 200.000, steps: 200\n",
            "Episode 98: reward: 200.000, steps: 200\n",
            "Episode 99: reward: 200.000, steps: 200\n",
            "Episode 100: reward: 200.000, steps: 200\n",
            "200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiQWvbQna4d4",
        "outputId": "e8cc3a9c-4e8a-4aba-f035-bf4afd85370b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Reloading Agent From Memory"
      ],
      "metadata": {
        "id": "J608dgeAbcCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.save_weights('weights/dqn_weights.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "6qDFPtIEbJC3"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del dqn\n",
        "del env"
      ],
      "metadata": {
        "id": "3mr4ekk2bxwf"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "model = build_model(states, actions)\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq8Yhc38b3gd",
        "outputId": "309852a1-7782-43cb-9b42-7c4b2206a63a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.load_weights(\"weights/dqn_weights.h5f\")"
      ],
      "metadata": {
        "id": "JKmHaZW4cpmV"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = dqn.test(env, nb_episodes=10, visualize=True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRCgCfAacx3J",
        "outputId": "ac75ee57-2b35-4cda-fce1-39084c3a1202"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntYEjm7QfJR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}